{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU, LSTM, Bidirectional, Dense, Flatten, Conv1D, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from pickle import load\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pickle import dump\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'AMZN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('/kaggle/input/stock-tweets-for-sentiment-analysis-and-prediction/stock_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_tweets.shape)\n",
    "all_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_tweets[all_tweets['Stock Name'] == stock_name]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = df.copy()\n",
    "sent_df[\"sentiment_score\"] = ''\n",
    "sent_df[\"Negative\"] = ''\n",
    "sent_df[\"Neutral\"] = ''\n",
    "sent_df[\"Positive\"] = ''\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "for indx, row in sent_df.T.iteritems():\n",
    "    try:\n",
    "        sentence_i = unicodedata.normalize('NFKD', sent_df.loc[indx, 'Tweet'])\n",
    "        sentence_sentiment = sentiment_analyzer.polarity_scores(sentence_i)\n",
    "        sent_df.at[indx, 'sentiment_score'] = sentence_sentiment['compound']\n",
    "        sent_df.at[indx, 'Negative'] = sentence_sentiment['neg']\n",
    "        sent_df.at[indx, 'Neutral'] = sentence_sentiment['neu']\n",
    "        sent_df.at[indx, 'Positive'] = sentence_sentiment['pos']\n",
    "    except TypeError:\n",
    "        print (sent_df.loc[indexx, 'Tweet'])\n",
    "        print (indx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['Date'] = pd.to_datetime(sent_df['Date'])\n",
    "sent_df['Date'] = sent_df['Date'].dt.date\n",
    "sent_df = sent_df.drop(columns=['Negative', 'Positive', 'Neutral', 'Stock Name', 'Company Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X12sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m sent_df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sent_df' is not defined"
     ]
    }
   ],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = sent_df.groupby([sent_df['Date']]).mean()\n",
    "print(twitter_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stocks = pd.read_csv('/kaggle/input/stock-tweets-for-sentiment-analysis-and-prediction/stock_yfinance_data.csv')\n",
    "print(all_stocks.shape)\n",
    "all_stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = all_stocks[all_stocks['Stock Name'] == stock_name]\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "stock_df['Date'] = stock_df['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = stock_df.join(twitter_df, how=\"left\", on=\"Date\")\n",
    "final_df = final_df.drop(columns=['Stock Name'])\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.plot(final_df['Date'], final_df['Close'], color='#008B8B')\n",
    "ax.set(xlabel=\"Date\", ylabel=\"USD\", title=f\"{stock_name} Stock Price\")\n",
    "ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tech_ind(data):\n",
    "    data['MA7'] = data.iloc[:,4].rolling(window=7).mean() #Close column\n",
    "    data['MA20'] = data.iloc[:,4].rolling(window=20).mean() #Close Column\n",
    "\n",
    "    data['MACD'] = data.iloc[:,4].ewm(span=26).mean() - data.iloc[:,1].ewm(span=12,adjust=False).mean()\n",
    "    #This is the difference of Closing price and Opening Price\n",
    "\n",
    "    # Create Bollinger Bands\n",
    "    data['20SD'] = data.iloc[:, 4].rolling(20).std()\n",
    "    data['upper_band'] = data['MA20'] + (data['20SD'] * 2)\n",
    "    data['lower_band'] = data['MA20'] - (data['20SD'] * 2)\n",
    "\n",
    "    # Create Exponential moving average\n",
    "    data['EMA'] = data.iloc[:,4].ewm(com=0.5).mean()\n",
    "\n",
    "    # Create LogMomentum\n",
    "    data['logmomentum'] = np.log(data.iloc[:,4] - 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_df = get_tech_ind(final_df)\n",
    "dataset = tech_df.iloc[20:,:].reset_index(drop=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tech_ind(dataset):\n",
    "    fig,ax = plt.subplots(figsize=(15, 8), dpi = 200)\n",
    "    x_ = range(3, dataset.shape[0])\n",
    "    x_ = list(dataset.index)\n",
    "\n",
    "    ax.plot(dataset['Date'], dataset['MA7'], label='Moving Average (7 days)', color='g', linestyle='--')\n",
    "    ax.plot(dataset['Date'], dataset['Close'], label='Closing Price', color='#6A5ACD')\n",
    "    ax.plot(dataset['Date'], dataset['MA20'], label='Moving Average (20 days)', color='r', linestyle='-.')\n",
    "    ax.xaxis.set_major_formatter(DateFormatter(\"%Y\"))\n",
    "    plt.title('Technical indicators')\n",
    "    plt.ylabel('Close (USD)')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_ind(tech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:, 1:] = pd.concat([dataset.iloc[:, 1:].ffill()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_series = pd.to_datetime(dataset['Date'])\n",
    "datetime_index = pd.DatetimeIndex(datetime_series.values)\n",
    "dataset = dataset.set_index(datetime_index)\n",
    "dataset = dataset.sort_values(by='Date')\n",
    "dataset = dataset.drop(columns='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, range, target_column):\n",
    "\n",
    "    '''\n",
    "    df: dataframe object\n",
    "    range: type tuple -> (lower_bound, upper_bound)\n",
    "        lower_bound: int\n",
    "        upper_bound: int\n",
    "    target_column: type str -> should reflect closing price of stock\n",
    "    '''\n",
    "\n",
    "    target_df_series = pd.DataFrame(df[target_column])\n",
    "    data = pd.DataFrame(df.iloc[:, :])\n",
    "\n",
    "    X_scaler = MinMaxScaler(feature_range=range)\n",
    "    y_scaler = MinMaxScaler(feature_range=range)\n",
    "    X_scaler.fit(data)\n",
    "    y_scaler.fit(target_df_series)\n",
    "\n",
    "    X_scale_dataset = X_scaler.fit_transform(data)\n",
    "    y_scale_dataset = y_scaler.fit_transform(target_df_series)\n",
    "    \n",
    "    dump(X_scaler, open('X_scaler.pkl', 'wb'))\n",
    "    dump(y_scaler, open('y_scaler.pkl', 'wb'))\n",
    "\n",
    "    return (X_scale_dataset,y_scale_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x_data,y_data, batch_size, predict_period):\n",
    "    X_batched, y_batched, yc = list(), list(), list()\n",
    "\n",
    "    for i in range(0,len(x_data),1):\n",
    "        x_value = x_data[i: i + batch_size][:, :]\n",
    "        y_value = y_data[i + batch_size: i + batch_size + predict_period][:, 0]\n",
    "        yc_value = y_data[i: i + batch_size][:, :]\n",
    "        if len(x_value) == batch_size and len(y_value) == predict_period:\n",
    "            X_batched.append(x_value)\n",
    "            y_batched.append(y_value)\n",
    "            yc.append(yc_value)\n",
    "\n",
    "    return np.array(X_batched), np.array(y_batched), np.array(yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data):\n",
    "    train_size = len(data) - 20\n",
    "    data_train = data[0:train_size]\n",
    "    data_test = data[train_size:]\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_index(dataset, X_train, batch_size, prediction_period):\n",
    "\n",
    "    # get the predict data (remove the in_steps days)\n",
    "    train_predict_index = dataset.iloc[batch_size: X_train.shape[0] + batch_size + prediction_period, :].index\n",
    "    test_predict_index = dataset.iloc[X_train.shape[0] + batch_size:, :].index\n",
    "\n",
    "    return train_predict_index, test_predict_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale_dataset,y_scale_dataset = normalize_data(dataset, (-1,1), \"Close\")\n",
    "X_batched, y_batched, yc = batch_data(X_scale_dataset, y_scale_dataset, batch_size = 5, predict_period = 1)\n",
    "print(\"X shape:\", X_batched.shape)\n",
    "print(\"y shape:\", y_batched.shape)\n",
    "print(\"yc shape:\", yc.shape)\n",
    "\n",
    "X_train, X_test, = split_train_test(X_batched)\n",
    "y_train, y_test, = split_train_test(y_batched)\n",
    "yc_train, yc_test, = split_train_test(yc)\n",
    "index_train, index_test, = predict_index(dataset, X_train, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1] \n",
    "feature_size = X_train.shape[2] \n",
    "output_dim = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(input_dim, output_dim, feature_size):\n",
    "    model = tf.keras.Sequential([LSTM(units = 1024, return_sequences = True, \n",
    "                                    input_shape=(input_dim, feature_size),recurrent_dropout = 0.3),\n",
    "                               LSTM(units = 512, return_sequences = True, recurrent_dropout = 0.3),\n",
    "                               LSTM(units = 256, return_sequences = True, recurrent_dropout = 0.3),\n",
    "                               LSTM(units = 128, return_sequences = True, recurrent_dropout = 0.3),\n",
    "                               LSTM(units = 64, recurrent_dropout = 0.3),\n",
    "                               Dense(32),\n",
    "                               Dense(16),\n",
    "                               Dense(8),\n",
    "                               Dense(units=output_dim)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(input_dim):\n",
    "    cnn_net = tf.keras.Sequential()\n",
    "    cnn_net.add(Conv1D(8, input_shape=(input_dim+1, 1), kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
    "    cnn_net.add(Conv1D(16, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
    "    cnn_net.add(Conv1D(32, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
    "    cnn_net.add(Conv1D(64, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
    "    cnn_net.add(Conv1D(128, kernel_size=1, strides=2, padding='same', activation=LeakyReLU(alpha=0.01)))\n",
    "    #cnn_net.add(Flatten())\n",
    "    cnn_net.add(LeakyReLU())\n",
    "    cnn_net.add(Dense(220, use_bias=False))\n",
    "    cnn_net.add(LeakyReLU())\n",
    "    cnn_net.add(Dense(220, use_bias=False, activation='relu'))\n",
    "    cnn_net.add(Dense(1, activation='sigmoid'))\n",
    "    return cnn_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    loss_f = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = loss_f(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = loss_f(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    loss_f = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    loss = loss_f(tf.ones_like(fake_output), fake_output)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "def train_step(real_x, real_y, yc, generator, discriminator, g_optimizer, d_optimizer):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(real_x, training=True)\n",
    "        generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
    "        d_fake_input = tf.concat([tf.cast(generated_data_reshape, tf.float64), yc], axis=1)\n",
    "        real_y_reshape = tf.reshape(real_y, [real_y.shape[0], real_y.shape[1], 1])\n",
    "        d_real_input = tf.concat([real_y_reshape, yc], axis=1)\n",
    "\n",
    "        real_output = discriminator(d_real_input, training=True)\n",
    "        fake_output = discriminator(d_fake_input, training=True)\n",
    "\n",
    "        g_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    g_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    d_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return real_y, generated_data, {'d_loss': disc_loss, 'g_loss': g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(real_x, real_y, yc, Epochs, generator, discriminator, g_optimizer, d_optimizer, checkpoint = 50):\n",
    "    train_info = {}\n",
    "    train_info[\"discriminator_loss\"] = []\n",
    "    train_info[\"generator_loss\"] = []\n",
    "\n",
    "    for epoch in tqdm(range(Epochs)):\n",
    "        real_price, fake_price, loss = train_step(real_x, real_y, yc, generator, discriminator, g_optimizer, d_optimizer)\n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        Real_price = []\n",
    "        Predicted_price = []\n",
    "        D_losses.append(loss['d_loss'].numpy())\n",
    "        G_losses.append(loss['g_loss'].numpy())\n",
    "        Predicted_price.append(fake_price.numpy())\n",
    "        Real_price.append(real_price.numpy())\n",
    "\n",
    "        #Save model every X checkpoints\n",
    "        if (epoch + 1) % checkpoint == 0:\n",
    "            tf.keras.models.save_model(generator, f'./models_gan/{stock_name}/generator_V_%d.h5' % epoch)\n",
    "            tf.keras.models.save_model(discriminator, f'./models_gan/{stock_name}/discriminator_V_%d.h5' % epoch)\n",
    "            print('epoch', epoch + 1, 'discriminator_loss', loss['d_loss'].numpy(), 'generator_loss', loss['g_loss'].numpy())\n",
    "    \n",
    "        train_info[\"discriminator_loss\"].append(D_losses)\n",
    "        train_info[\"generator_loss\"].append(G_losses)\n",
    "  \n",
    "    Predicted_price = np.array(Predicted_price)\n",
    "    Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])\n",
    "    Real_price = np.array(Real_price)\n",
    "    Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(train_info[\"discriminator_loss\"], label='Disc_loss', color='#000000')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Discriminator Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(train_info[\"generator_loss\"], label='Gen_loss', color='#000000')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Generator Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return Predicted_price, Real_price, np.sqrt(mean_squared_error(Real_price, Predicted_price)) / np.mean(Real_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(Real_price, Predicted_price, index_train):\n",
    "    X_scaler = load(open('/content/X_scaler.pkl', 'rb'))\n",
    "    y_scaler = load(open('/content/y_scaler.pkl', 'rb'))\n",
    "    train_predict_index = index_train\n",
    "\n",
    "    rescaled_Real_price = y_scaler.inverse_transform(Real_price)\n",
    "    rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_price)\n",
    "\n",
    "    predict_result = pd.DataFrame()\n",
    "    for i in range(rescaled_Predicted_price.shape[0]):\n",
    "        y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=[\"predicted_price\"], index=train_predict_index[i:i+output_dim])\n",
    "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
    "  \n",
    "    real_price = pd.DataFrame()\n",
    "    for i in range(rescaled_Real_price.shape[0]):\n",
    "        y_train = pd.DataFrame(rescaled_Real_price[i], columns=[\"real_price\"], index=train_predict_index[i:i+output_dim])\n",
    "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
    "  \n",
    "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
    "    real_price['real_mean'] = real_price.mean(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(real_price[\"real_mean\"])\n",
    "    plt.plot(predict_result[\"predicted_mean\"], color = 'r')\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Stock price\")\n",
    "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
    "    plt.title(\"The result of Training\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    predicted = predict_result[\"predicted_mean\"]\n",
    "    real = real_price[\"real_mean\"]\n",
    "    For_MSE = pd.concat([predicted, real], axis = 1)\n",
    "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
    "    print('-- Train RMSE -- ', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Code\n",
    "\n",
    "@tf.function \n",
    "\n",
    "def exe_op(generator, real_x):\n",
    "    generated_data = generator(real_x, training = False)\n",
    "\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_data(Real_test_price, Predicted_test_price, index_test):\n",
    "    X_scaler = load(open('X_scaler.pkl', 'rb'))\n",
    "    y_scaler = load(open('y_scaler.pkl', 'rb'))\n",
    "    test_predict_index = index_test\n",
    "\n",
    "    rescaled_Real_price = y_scaler.inverse_transform(Real_test_price)\n",
    "    rescaled_Predicted_price = y_scaler.inverse_transform(Predicted_test_price)\n",
    "\n",
    "    predict_result = pd.DataFrame()\n",
    "    for i in range(rescaled_Predicted_price.shape[0]):\n",
    "        y_predict = pd.DataFrame(rescaled_Predicted_price[i], columns=[\"predicted_price\"], index=test_predict_index[i:i+output_dim])\n",
    "        predict_result = pd.concat([predict_result, y_predict], axis=1, sort=False)\n",
    "  \n",
    "    real_price = pd.DataFrame()\n",
    "    for i in range(rescaled_Real_price.shape[0]):\n",
    "        y_train = pd.DataFrame(rescaled_Real_price[i], columns=[\"real_price\"], index=test_predict_index[i:i+output_dim])\n",
    "        real_price = pd.concat([real_price, y_train], axis=1, sort=False)\n",
    "  \n",
    "    predict_result['predicted_mean'] = predict_result.mean(axis=1)\n",
    "    real_price['real_mean'] = real_price.mean(axis=1)\n",
    "\n",
    "    predicted = predict_result[\"predicted_mean\"]\n",
    "    real = real_price[\"real_mean\"]\n",
    "    For_MSE = pd.concat([predicted, real], axis = 1)\n",
    "    RMSE = np.sqrt(mean_squared_error(predicted, real))\n",
    "    print('Test RMSE: ', RMSE)\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(real_price[\"real_mean\"], color='#00008B')\n",
    "    plt.plot(predict_result[\"predicted_mean\"], color = '#8B0000', linestyle='--')\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Stock price\")\n",
    "    plt.legend((\"Real price\", \"Predicted price\"), loc=\"upper left\", fontsize=16)\n",
    "    plt.title(f\"Prediction on test data for {stock_name}\", fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "epochs = 500\n",
    "\n",
    "g_optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "d_optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "\n",
    "generator = make_generator_model(X_train.shape[1], output_dim, X_train.shape[2])\n",
    "discriminator = make_discriminator_model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(generator, to_file='generator_keras_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(discriminator, to_file='discriminator_keras_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hp\\Desktop\\stock port\\Stock-price-predection-using-LSTM-and-Sentiment-analysis\\code.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hp/Desktop/stock%20port/Stock-price-predection-using-LSTM-and-Sentiment-analysis/code.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predicted_price, real_price, RMSPE \u001b[39m=\u001b[39m train(X_train, y_train, yc_train, epochs, generator, discriminator, g_optimizer, d_optimizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_price, real_price, RMSPE = train(X_train, y_train, yc_train, epochs, generator, discriminator, g_optimizer, d_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = tf.keras.models.load_model(f'./models_gan/{stock_name}/generator_V_{epochs-1}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(X_test,plot_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
